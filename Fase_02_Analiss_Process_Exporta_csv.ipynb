{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zeuspyEC/TikTokAnalysis/blob/main/Fase_02_Analiss_Process_Exporta_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bf3b4d-1584-4cd7-8528-097999885606",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73bf3b4d-1584-4cd7-8528-097999885606",
        "outputId": "f8dfc36d-76f9-45a0-8919-94c5aa719d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping mediapipe as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: opencv-python 4.10.0.84\n",
            "Uninstalling opencv-python-4.10.0.84:\n",
            "  Successfully uninstalled opencv-python-4.10.0.84\n",
            "Found existing installation: protobuf 4.25.5\n",
            "Uninstalling protobuf-4.25.5:\n",
            "  Successfully uninstalled protobuf-4.25.5\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n"
          ]
        }
      ],
      "source": [
        "# Celda 1: Desinstalar todo primero\n",
        "!pip uninstall -y mediapipe opencv-python protobuf numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6a4d39-b020-4ee3-958e-f0b142b050c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b6a4d39-b020-4ee3-958e-f0b142b050c6",
        "outputId": "c4f5d9dd-0167-41bd-ea3a-8b3c4a169d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m51.2/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "imgaug 0.4.0 requires opencv-python, which is not installed.\n",
            "orbax-checkpoint 0.6.4 requires protobuf, which is not installed.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, which is not installed.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, which is not installed.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, which is not installed.\n",
            "tensorflow-hub 0.16.1 requires protobuf>=3.19.6, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting opencv-python==4.8.1.78\n",
            "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.8.1.78) (1.26.4)\n",
            "Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.8.1.78\n"
          ]
        }
      ],
      "source": [
        "# Celda 2: Instalar las dependencias en este orden específico\n",
        "!pip install numpy==1.26.4\n",
        "#!pip install protobuf==3.20.3\n",
        "!pip install opencv-python==4.8.1.78"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58d8a017-ceee-44d5-965e-2ebb5a730048",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "58d8a017-ceee-44d5-965e-2ebb5a730048",
        "outputId": "9a8dbf4b-626e-4e11-c761-907991b08697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mediapipe-python'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 45 (delta 14), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (45/45), 31.45 MiB | 18.95 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.18 protobuf-4.25.5 sounddevice-0.5.1\n",
            "Collecting PyQt5\n",
            "  Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting PyQt5-sip<13,>=12.15 (from PyQt5)\n",
            "  Downloading PyQt5_sip-12.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (421 bytes)\n",
            "Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from PyQt5)\n",
            "  Downloading PyQt5_Qt5-5.15.15-py3-none-manylinux2014_x86_64.whl.metadata (536 bytes)\n",
            "Downloading PyQt5-5.15.11-cp38-abi3-manylinux_2_17_x86_64.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQt5_Qt5-5.15.15-py3-none-manylinux2014_x86_64.whl (59.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyQt5_sip-12.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.whl (270 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.5/270.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyQt5-Qt5, PyQt5-sip, PyQt5\n",
            "Successfully installed PyQt5-5.15.11 PyQt5-Qt5-5.15.15 PyQt5-sip-12.15.0\n",
            "Collecting ipython==7.32.0\n",
            "  Downloading ipython-7.32.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython==7.32.0)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==7.32.0) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==7.32.0) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==7.32.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.32.0) (0.2.13)\n",
            "Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m793.9/793.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipython\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ipython-7.32.0 jedi-0.19.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython"
                ]
              },
              "id": "451b6c5c1e314f188fb0944528f5e77a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/AssemblyAI-Examples/mediapipe-python.git\n",
        "!pip install mediapipe\n",
        "!pip install PyQt5\n",
        "!pip install ipython==7.32.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy librosa opencv-python mediapipe scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9gpeXiAH7nM",
        "outputId": "a3fc62b3-cc21-4b0d-87fb-9e24fd4a32f5"
      },
      "id": "B9gpeXiAH7nM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.1.78)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.8.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3082, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3146, in __init__\n",
            "    str(self.marker) if self.marker else None,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 287, in __str__\n",
            "    return _format_marker(self._markers)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 147, in _format_marker\n",
            "    isinstance(marker, list)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 172, in emit\n",
            "    style = Style(color=\"red\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/style.py\", line 151, in __init__\n",
            "    self._set_attributes = sum(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade opencv-python numpy matplotlib protobuf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzaNMdx9IAtk",
        "outputId": "807d0a2a-b682-4ff7-e805-4a11d38a8178"
      },
      "id": "xzaNMdx9IAtk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.1.78)\n",
            "Collecting opencv-python\n",
            "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.25.5)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, opencv-python, matplotlib\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.1.78\n",
            "    Uninstalling opencv-python-4.8.1.78:\n",
            "      Successfully uninstalled opencv-python-4.8.1.78\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
            "langchain 0.3.7 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.3 which is incompatible.\n",
            "mediapipe 0.10.18 requires numpy<2, but you have numpy 2.1.3 which is incompatible.\n",
            "mediapipe 0.10.18 requires protobuf<5,>=4.25.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.1.3 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.9.2 numpy-2.1.3 opencv-python-4.10.0.84 protobuf-5.28.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mediapipe numpy protobuf opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iimh9hKyIDCB",
        "outputId": "8d3cfed9-82fd-462d-b5ff-053bcf8de8fe"
      },
      "id": "iimh9hKyIDCB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (5.28.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.9.2)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Collecting protobuf\n",
            "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf, numpy\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.28.3\n",
            "    Uninstalling protobuf-5.28.3:\n",
            "      Successfully uninstalled protobuf-5.28.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "Successfully installed numpy-1.26.4 protobuf-4.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit pandas numpy librosa opencv-python mediapipe scikit-learn tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsccQNquIFps",
        "outputId": "49a3e305-72b8-436b-dc3e-449f36e7687b"
      },
      "id": "IsccQNquIFps",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.18)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.9.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Downloading streamlit-1.40.1-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf6a4a4-64cf-482c-88f2-1946248d5da4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf6a4a4-64cf-482c-88f2-1946248d5da4",
        "outputId": "91683d77-4712-408f-b3e0-ad3bd0167689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MediaPipe importado correctamente\n"
          ]
        }
      ],
      "source": [
        "# Celda 3: Reiniciar el kernel y probar\n",
        "import mediapipe as mp\n",
        "print(\"MediaPipe importado correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h\n",
        "!ls /\n",
        "!ls -lh /content/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mount\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUT4kLHZJQvy",
        "outputId": "e81d6895-3b2c-49e4-bf6d-e256a673996e"
      },
      "id": "IUT4kLHZJQvy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   34G   75G  31% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  820M  59% /usr/sbin/docker-init\n",
            "tmpfs           6.4G  3.5M  6.4G   1% /var/colab\n",
            "/dev/sda1        77G   58G   20G  75% /etc/hosts\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "bin\t\t\t    dev    lib64\t\t     opt\t\trun   tools\n",
            "boot\t\t\t    etc    libx32\t\t     proc\t\tsbin  usr\n",
            "content\t\t\t    home   media\t\t     python-apt\t\tsrv   var\n",
            "cuda-keyring_1.0-1_all.deb  lib    mnt\t\t\t     python-apt.tar.xz\tsys\n",
            "datalab\t\t\t    lib32  NGC-DL-CONTAINER-LICENSE  root\t\ttmp\n",
            "total 41M\n",
            "-rw-r--r-- 1 root root 5.4M Nov 13 20:08 7428376797579840773.mp4\n",
            "-rw-r--r-- 1 root root 5.8M Nov 13 20:08 7428436341530086662.mp4\n",
            "-rw-r--r-- 1 root root  18M Nov 13 20:08 7428647385791778053.mp4\n",
            "-rw-r--r-- 1 root root 7.3M Nov 13 20:08 7428958018512964870.mp4\n",
            "-rw-r--r-- 1 root root 4.7M Nov 13 20:08 7429081103891303686.mp4\n",
            "drwxr-xr-x 3 root root 4.0K Nov 13 20:05 mediapipe-python\n",
            "drwxr-xr-x 2 root root 4.0K Nov 13 20:07 models\n",
            "drwxr-xr-x 1 root root 4.0K Nov 12 14:25 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc91434-0d07-4a92-b515-900a6cb84fcd",
      "metadata": {
        "scrolled": true,
        "id": "5cc91434-0d07-4a92-b515-900a6cb84fcd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import librosa\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import logging\n",
        "import json\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "import pickle\n",
        "from scipy.spatial import distance\n",
        "import math\n",
        "import time\n",
        "import traceback\n",
        "import pytesseract\n",
        "import shutil\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from tensorflow.keras.models import load_model  # Agregar este import\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class AnalysisConfig:\n",
        "    \"\"\"Configuración para el análisis de videos\"\"\"\n",
        "    # Configuración general\n",
        "    sample_rate: int = 16000\n",
        "    segment_duration: float = 2.5\n",
        "    min_detection_confidence: float = 0.5\n",
        "    use_gpu: bool = torch.cuda.is_available()\n",
        "    batch_size: int = 32\n",
        "    cache_dir: str = \".cache\"\n",
        "\n",
        "    # Parámetros adicionales necesarios\n",
        "    n_fft: int = 2048\n",
        "    hop_length: int = 512\n",
        "    mfcc_coeffs: int = 13\n",
        "    eye_ar_thresh: float = 0.3\n",
        "    mouth_ar_thresh: float = 0.6\n",
        "\n",
        "    # Rutas de modelos\n",
        "    model_paths: Dict[str, str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validación de configuración y setup de paths\"\"\"\n",
        "        if self.segment_duration <= 0:\n",
        "            raise ValueError(\"segment_duration debe ser positivo\")\n",
        "        if not 0 < self.min_detection_confidence <= 1:\n",
        "            raise ValueError(\"min_detection_confidence debe estar entre 0 y 1\")\n",
        "\n",
        "        # Setup de paths por defecto si no se especifican\n",
        "        if self.model_paths is None:\n",
        "            base_path = Path(__file__).parent / \"models\"\n",
        "            self.model_paths = {\n",
        "                'cnn': str(base_path / \"cnn_model.h5\"),\n",
        "                'text': str(base_path / \"text_model.pkl\")\n",
        "            }\n",
        "\n",
        "\n",
        "# Constantes para análisis facial\n",
        "FACIAL_LANDMARKS = {\n",
        "    'LEFT_EYE': [362, 385, 387, 263, 373, 380],\n",
        "    'RIGHT_EYE': [33, 160, 158, 133, 153, 144],\n",
        "    'MOUTH': [78, 81, 13, 311, 308, 402, 14, 178],\n",
        "    'BROWS': [276, 282, 283, 293, 296, 300],\n",
        "    'NOSE': [1],\n",
        "    'LEFT_MOUTH': [61],\n",
        "    'RIGHT_MOUTH': [291]\n",
        "}\n",
        "\n",
        "# Constantes para análisis de emociones\n",
        "EMOTION_WEIGHTS = {\n",
        "    'HAPPINESS': {\n",
        "        'mouth': 0.7,\n",
        "        'eye': 0.3\n",
        "    },\n",
        "    'SADNESS': {\n",
        "        'mouth': 0.6,\n",
        "        'brow': 0.4\n",
        "    },\n",
        "    'ANGER': {\n",
        "        'brow': 0.7,\n",
        "        'eye': 0.3\n",
        "    },\n",
        "    'SURPRISE': {\n",
        "        'eye': 0.5,\n",
        "        'mouth': 0.5\n",
        "    }\n",
        "}\n",
        "\n",
        "# Constantes para análisis de audio\n",
        "AUDIO_FEATURES = {\n",
        "    'MFCC_DELTA_WINDOW': 2,\n",
        "    'PITCH_FILTER_THRESHOLD': 0.1,\n",
        "    'ENERGY_FRAME_LENGTH': 2048,\n",
        "    'ENERGY_HOP_LENGTH': 512,\n",
        "    'ZCR_FRAME_LENGTH': 2048,\n",
        "    'ZCR_HOP_LENGTH': 512,\n",
        "    'SPECTRAL_ROLLOFF_ROLL_PERCENT': 0.85\n",
        "}\n",
        "\n",
        "# Constantes para análisis de engagement\n",
        "ENGAGEMENT_THRESHOLDS = {\n",
        "    'LOW': 0.3,\n",
        "    'MEDIUM': 0.6,\n",
        "    'HIGH': 0.8\n",
        "}\n",
        "\n",
        "# Constantes para análisis de atención\n",
        "ATTENTION_WEIGHTS = {\n",
        "    'EYE_ATTENTION': 0.7,\n",
        "    'HEAD_POSE': 0.3\n",
        "}\n",
        "\n",
        "# Constantes para recomendaciones\n",
        "RECOMMENDATION_THRESHOLDS = {\n",
        "    'ENGAGEMENT': 0.5,\n",
        "    'EMOTIONAL_CONSISTENCY': 0.7,\n",
        "    'ATTENTION': 0.6\n",
        "}\n",
        "\n",
        "# Configuración de cache\n",
        "CACHE_CONFIG = {\n",
        "    'USE_CACHE': True,\n",
        "    'CACHE_VERSION': '1.0',\n",
        "    'MAX_CACHE_SIZE': 1024 * 1024 * 100  # 100 MB\n",
        "}\n",
        "\n",
        "# Configuración de procesamiento\n",
        "PROCESSING_CONFIG = {\n",
        "    'MAX_VIDEO_DURATION': 300,  # 5 minutos\n",
        "    'MIN_VIDEO_DURATION': 1,    # 1 segundo\n",
        "    'MAX_FRAME_WIDTH': 1280,\n",
        "    'MAX_FRAME_HEIGHT': 720,\n",
        "    'MIN_FACE_SIZE': 100,\n",
        "    'MAX_BATCH_SIZE': 32\n",
        "}\n",
        "\n",
        "# Tipos personalizados\n",
        "VideoFeatures = Dict[str, Union[np.ndarray, Dict[str, float]]]\n",
        "AudioFeatures = Dict[str, Union[np.ndarray, float, Dict[str, float]]]\n",
        "AnalysisResults = Dict[str, Union[str, float, Dict[str, float]]]\n",
        "\n",
        "class VideoAnalyzer:\n",
        "    \"\"\"Analiza los aspectos visuales del video\"\"\"\n",
        "\n",
        "    def __init__(self, videos_path: str, output_path: str, config: 'AnalysisConfig'):\n",
        "        \"\"\"\n",
        "        Inicializa el analizador de video.\n",
        "\n",
        "        Args:\n",
        "            videos_path: Ruta al directorio con los videos\n",
        "            output_path: Ruta donde se guardarán los resultados\n",
        "            config: Configuración del análisis\n",
        "        \"\"\"\n",
        "        self.videos_path = Path(videos_path)\n",
        "        self.output_path = Path(output_path)\n",
        "        self.config = config\n",
        "\n",
        "        # Configurar logging\n",
        "        self.logger = self._setup_logging()\n",
        "\n",
        "        # Validar y crear directorios\n",
        "        self._setup_directories()\n",
        "\n",
        "        # Inicializar componentes\n",
        "        self._initialize_components()\n",
        "\n",
        "    def _setup_logging(self) -> logging.Logger:\n",
        "        \"\"\"Configura el sistema de logging para el analizador de video\"\"\"\n",
        "        logger = logging.getLogger(f\"{__name__}.VideoAnalyzer\")\n",
        "\n",
        "        if not logger.handlers:\n",
        "            # Configurar formato\n",
        "            formatter = logging.Formatter(\n",
        "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "            )\n",
        "\n",
        "            # Crear archivo de log\n",
        "            log_file = self.output_path / 'video_analysis.log'\n",
        "            file_handler = logging.FileHandler(log_file)\n",
        "            file_handler.setFormatter(formatter)\n",
        "            file_handler.setLevel(logging.INFO)\n",
        "\n",
        "            # Handler de consola\n",
        "            console_handler = logging.StreamHandler()\n",
        "            console_handler.setFormatter(formatter)\n",
        "            console_handler.setLevel(logging.INFO)\n",
        "\n",
        "            # Agregar handlers al logger\n",
        "            logger.addHandler(file_handler)\n",
        "            logger.addHandler(console_handler)\n",
        "            logger.setLevel(logging.INFO)\n",
        "\n",
        "        return logger\n",
        "\n",
        "    def _setup_directories(self):\n",
        "        \"\"\"Configura y valida directorios necesarios\"\"\"\n",
        "        # Verificar directorio de videos\n",
        "        if not self.videos_path.exists():\n",
        "            raise FileNotFoundError(f\"Videos directory not found: {self.videos_path}\")\n",
        "\n",
        "        # Crear directorio de salida si no existe\n",
        "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Crear subdirectorios necesarios\n",
        "        results_dir = self.output_path / \"results\"\n",
        "        cache_dir = self.output_path / \".cache\"\n",
        "        log_dir = self.output_path / \"logs\"\n",
        "\n",
        "        for directory in [results_dir, cache_dir, log_dir]:\n",
        "            directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Crear o validar directorio de cache si es necesario\n",
        "        if self.config.cache_dir:\n",
        "            cache_path = Path(self.config.cache_dir)\n",
        "            cache_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.logger.info(f\"Directories setup completed:\")\n",
        "        self.logger.info(f\"- Videos: {self.videos_path}\")\n",
        "        self.logger.info(f\"- Output: {self.output_path}\")\n",
        "        self.logger.info(f\"- Cache: {cache_path if self.config.cache_dir else 'Not used'}\")\n",
        "\n",
        "    def _initialize_components(self):\n",
        "        \"\"\"Inicializa componentes necesarios\"\"\"\n",
        "        try:\n",
        "            # Inicializar MediaPipe\n",
        "            self.mp_face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "                static_image_mode=False,\n",
        "                max_num_faces=1,\n",
        "                refine_landmarks=True,\n",
        "                min_detection_confidence=self.config.min_detection_confidence\n",
        "            )\n",
        "\n",
        "            self.logger.info(\"Components initialized successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error initializing components: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_model(self, model_type: str):\n",
        "        \"\"\"Carga un modelo específico\"\"\"\n",
        "        try:\n",
        "            model_path = self.config.model_paths.get(model_type)\n",
        "            if not model_path or not Path(model_path).exists():\n",
        "                logger.warning(f\"{model_type} model not found at {model_path}\")\n",
        "                return None\n",
        "\n",
        "            if model_type == 'cnn':\n",
        "                return load_model(model_path)\n",
        "            elif model_type == 'text':\n",
        "                with open(model_path, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading {model_type} model: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_video_features(self, video_path: Path) -> Optional[Dict]:\n",
        "        \"\"\"Extrae características del video\"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(str(video_path))\n",
        "            if not cap.isOpened():\n",
        "                raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "            features = []\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                frame_features = self.extract_features(frame)\n",
        "                if frame_features:\n",
        "                    features.append(frame_features)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            if not features:\n",
        "                return None\n",
        "\n",
        "            return {\n",
        "                'frame_features': features,\n",
        "                'avg_features': self._calculate_average_features(features)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting video features: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _save_results(self, results: Dict):\n",
        "        \"\"\"Guarda los resultados del análisis\"\"\"\n",
        "        try:\n",
        "            results_file = self.output_path / \"results\" / f\"{results['video_id']}_analysis.json\"\n",
        "            with results_file.open('w') as f:\n",
        "                json.dump(results, f, indent=4)\n",
        "\n",
        "            self.logger.info(f\"Results saved to {results_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving results: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_video(self, video_path: Path) -> Optional[Dict]:\n",
        "        \"\"\"Procesa un video individual\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Processing video: {video_path.name}\")\n",
        "\n",
        "            # Validar video\n",
        "            if not video_path.exists():\n",
        "                raise FileNotFoundError(f\"Video not found: {video_path}\")\n",
        "\n",
        "            # Extraer features\n",
        "            video_features = self._extract_video_features(video_path)\n",
        "\n",
        "            if video_features is None:\n",
        "                return None\n",
        "\n",
        "            # Generar resultados\n",
        "            results = {\n",
        "                'video_id': video_path.stem,\n",
        "                'features': video_features\n",
        "            }\n",
        "\n",
        "            # Guardar resultados\n",
        "            self._save_results(results)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing video {video_path.name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _initialize_mediapipe(self):\n",
        "        \"\"\"Inicializa los componentes de MediaPipe\"\"\"\n",
        "        try:\n",
        "            self.mp_face_mesh = mp.solutions.face_mesh\n",
        "            self.mp_drawing = mp.solutions.drawing_utils\n",
        "            self.mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "            self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
        "                static_image_mode=False,\n",
        "                max_num_faces=1,\n",
        "                refine_landmarks=True,\n",
        "                min_detection_confidence=self.config.min_detection_confidence\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing MediaPipe: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _analyze_emotions(self, eye_ratio: float, mouth_ratio: float, brow_ratio: float) -> Dict[str, float]:\n",
        "        \"\"\"Analiza emociones basándose en características faciales\"\"\"\n",
        "        try:\n",
        "            # Calcular scores individuales de emociones\n",
        "            happy_score = self._calculate_happiness(mouth_ratio, eye_ratio)\n",
        "            neutral_score = self._calculate_neutral(eye_ratio, mouth_ratio, brow_ratio)\n",
        "            sad_score = self._calculate_sadness(mouth_ratio, brow_ratio)\n",
        "            angry_score = self._calculate_anger(brow_ratio, eye_ratio)\n",
        "            surprised_score = self._calculate_surprise(eye_ratio, mouth_ratio)\n",
        "\n",
        "            # Crear diccionario de emociones\n",
        "            emotions = {\n",
        "                'happy': happy_score,\n",
        "                'neutral': neutral_score,\n",
        "                'sad': sad_score,\n",
        "                'angry': angry_score,\n",
        "                'surprised': surprised_score\n",
        "            }\n",
        "\n",
        "            # Normalizar scores\n",
        "            total = sum(emotions.values())\n",
        "            if total > 0:\n",
        "                emotions = {k: v/total for k, v in emotions.items()}\n",
        "\n",
        "            return emotions\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error analyzing emotions: {e}\")\n",
        "            return {\n",
        "                'happy': 0.0,\n",
        "                'neutral': 1.0,\n",
        "                'sad': 0.0,\n",
        "                'angry': 0.0,\n",
        "                'surprised': 0.0\n",
        "            }\n",
        "\n",
        "    def _calculate_face_size_ratio(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula la proporción del tamaño del rostro en relación al frame\"\"\"\n",
        "        try:\n",
        "            if len(landmarks) < 468 * 3:  # 468 landmarks * 3 coordenadas (x,y,z)\n",
        "                return 0.0\n",
        "\n",
        "            landmarks = landmarks.reshape(-1, 3)\n",
        "            face_width = np.max(landmarks[:, 0]) - np.min(landmarks[:, 0])\n",
        "            face_height = np.max(landmarks[:, 1]) - np.min(landmarks[:, 1])\n",
        "\n",
        "            # Normalizar a un valor entre 0 y 1\n",
        "            size_ratio = (face_width * face_height)\n",
        "            return float(np.clip(size_ratio, 0, 1))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating face size ratio: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_face_angle(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el ángulo del rostro\"\"\"\n",
        "        try:\n",
        "            if len(landmarks) < 468 * 3:\n",
        "                return 0.0\n",
        "\n",
        "            landmarks = landmarks.reshape(-1, 3)\n",
        "            # Usar puntos del mentón y la nariz para calcular el ángulo\n",
        "            chin = landmarks[152]\n",
        "            nose = landmarks[1]\n",
        "            angle = math.atan2(chin[1] - nose[1], chin[0] - nose[0])\n",
        "            return float(angle)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating face angle: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_gaze_direction(self, landmarks: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Calcula la dirección de la mirada\"\"\"\n",
        "        try:\n",
        "            if len(landmarks) < 468 * 3:\n",
        "                return {'x': 0.0, 'y': 0.0, 'z': 0.0}\n",
        "\n",
        "            landmarks = landmarks.reshape(-1, 3)\n",
        "            # Usar puntos de los ojos para estimar dirección de mirada\n",
        "            left_eye = np.mean(landmarks[FACIAL_LANDMARKS['LEFT_EYE']], axis=0)\n",
        "            right_eye = np.mean(landmarks[FACIAL_LANDMARKS['RIGHT_EYE']], axis=0)\n",
        "\n",
        "            return {\n",
        "                'x': float(right_eye[0] - left_eye[0]),\n",
        "                'y': float(right_eye[1] - left_eye[1]),\n",
        "                'z': float(right_eye[2] - left_eye[2])\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating gaze direction: {e}\")\n",
        "            return {'x': 0.0, 'y': 0.0, 'z': 0.0}\n",
        "\n",
        "    def extract_facial_features(self, frames: List[np.ndarray]) -> List[Dict]:\n",
        "        \"\"\"Extrae características faciales de una lista de frames\"\"\"\n",
        "        features = []\n",
        "        for frame in frames:\n",
        "            try:\n",
        "                if frame is None or frame.size == 0:\n",
        "                    features.append(self._get_empty_features())\n",
        "                    continue\n",
        "\n",
        "                frame_features = self._process_single_frame(frame)\n",
        "\n",
        "                # Agregar face_size_ratio si no existe\n",
        "                if 'face_size_ratio' not in frame_features:\n",
        "                    frame_features['face_size_ratio'] = self._calculate_face_size_ratio(frame_features.get('landmarks', []))\n",
        "\n",
        "                # Agregar face_angle si no existe\n",
        "                if 'face_angle' not in frame_features:\n",
        "                    frame_features['face_angle'] = self._calculate_face_angle(frame_features.get('landmarks', []))\n",
        "\n",
        "                # Agregar gaze_direction si no existe\n",
        "                if 'gaze_direction' not in frame_features:\n",
        "                    frame_features['gaze_direction'] = self._calculate_gaze_direction(frame_features.get('landmarks', []))\n",
        "\n",
        "                # Agregar expression_change_rate si no existe\n",
        "                if 'expression_change_rate' not in frame_features:\n",
        "                    frame_features['expression_change_rate'] = 0.0\n",
        "\n",
        "                # Agregar motion_vectors si no existe\n",
        "                if 'motion_vectors' not in frame_features:\n",
        "                    frame_features['motion_vectors'] = []\n",
        "\n",
        "                # Agregar camera_stability si no existe\n",
        "                if 'camera_stability' not in frame_features:\n",
        "                    frame_features['camera_stability'] = 1.0\n",
        "\n",
        "                # Agregar scene_complexity si no existe\n",
        "                if 'scene_complexity' not in frame_features:\n",
        "                    frame_features['scene_complexity'] = 0.5\n",
        "\n",
        "                # Agregar action_intensity si no existe\n",
        "                if 'action_intensity' not in frame_features:\n",
        "                    frame_features['action_intensity'] = 0.0\n",
        "\n",
        "                features.append(frame_features)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing frame: {e}\")\n",
        "                features.append(self._get_empty_features())\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _process_single_frame(self, frame: np.ndarray) -> Dict:\n",
        "        \"\"\"Procesa un único frame para extraer características faciales\"\"\"\n",
        "        if frame is None or frame.size == 0:\n",
        "            return self._get_empty_features()\n",
        "\n",
        "        # Convertir a RGB\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        try:\n",
        "            # Procesar con MediaPipe\n",
        "            results = self.face_mesh.process(rgb_frame)\n",
        "\n",
        "            if not results.multi_face_landmarks:\n",
        "                return self._get_empty_features()\n",
        "\n",
        "            # Extraer landmarks\n",
        "            landmarks = np.array([[lm.x, lm.y, lm.z]\n",
        "                                for lm in results.multi_face_landmarks[0].landmark])\n",
        "\n",
        "            # Calcular ratios\n",
        "            eye_ratio = self._calculate_eye_ratio(landmarks)\n",
        "            mouth_ratio = self._calculate_mouth_ratio(landmarks)\n",
        "            brow_ratio = self._calculate_brow_ratio(landmarks)\n",
        "\n",
        "            # Analizar emociones\n",
        "            emotions = self._analyze_emotions(eye_ratio, mouth_ratio, brow_ratio)\n",
        "\n",
        "            # Calcular características adicionales\n",
        "            features = {\n",
        "                'landmarks': landmarks.flatten(),\n",
        "                'emotions': emotions,\n",
        "                'attention_score': self._calculate_attention(landmarks),\n",
        "                'engagement_score': self._calculate_engagement(landmarks),\n",
        "                'eye_ratio': eye_ratio,\n",
        "                'mouth_ratio': mouth_ratio\n",
        "            }\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error processing frame: {e}\")\n",
        "            return self._get_empty_features()\n",
        "\n",
        "    def _get_empty_features(self) -> Dict:\n",
        "        \"\"\"Retorna un diccionario vacío de características faciales\"\"\"\n",
        "        return {\n",
        "            'landmarks': np.zeros(468 * 3),\n",
        "            'emotions': {\n",
        "                'happy': 0.0,\n",
        "                'neutral': 1.0,\n",
        "                'sad': 0.0,\n",
        "                'angry': 0.0,\n",
        "                'surprised': 0.0\n",
        "            },\n",
        "            'attention_score': 0.0,\n",
        "            'engagement_score': 0.0,\n",
        "            'eye_ratio': 0.0,\n",
        "            'mouth_ratio': 0.0\n",
        "        }\n",
        "\n",
        "    def _calculate_eye_ratio(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el ratio de apertura de los ojos\"\"\"\n",
        "        try:\n",
        "            left_eye = landmarks[FACIAL_LANDMARKS['LEFT_EYE']]\n",
        "            right_eye = landmarks[FACIAL_LANDMARKS['RIGHT_EYE']]\n",
        "\n",
        "            left_ear = self._eye_aspect_ratio(left_eye)\n",
        "            right_ear = self._eye_aspect_ratio(right_eye)\n",
        "\n",
        "            return (left_ear + right_ear) / 2\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating eye ratio: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _eye_aspect_ratio(self, eye_points) -> float:\n",
        "        \"\"\"Calcula el Eye Aspect Ratio\"\"\"\n",
        "        v1 = distance.euclidean(eye_points[1], eye_points[5])\n",
        "        v2 = distance.euclidean(eye_points[2], eye_points[4])\n",
        "        h = distance.euclidean(eye_points[0], eye_points[3])\n",
        "        ear = (v1 + v2) / (2.0 * h) if h > 0 else 0\n",
        "        return float(ear)\n",
        "\n",
        "    def _calculate_mouth_ratio(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el ratio de apertura de la boca\"\"\"\n",
        "        try:\n",
        "            mouth_points = landmarks[FACIAL_LANDMARKS['MOUTH']]\n",
        "\n",
        "            vertical = distance.euclidean(mouth_points[2], mouth_points[6])\n",
        "            horizontal = distance.euclidean(mouth_points[0], mouth_points[4])\n",
        "\n",
        "            return vertical / horizontal if horizontal > 0 else 0.0\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating mouth ratio: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_brow_ratio(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el ratio de elevación de las cejas\"\"\"\n",
        "        try:\n",
        "            brow_points = FACIAL_LANDMARKS['BROWS']\n",
        "            brow_landmarks = landmarks[brow_points]\n",
        "\n",
        "            elevation = np.mean(brow_landmarks[:, 1])\n",
        "            return float(elevation)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating brow ratio: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_happiness(self, mouth_ratio: float, eye_ratio: float) -> float:\n",
        "        \"\"\"Calcula el score de felicidad basado en ratios faciales\"\"\"\n",
        "        try:\n",
        "            mouth_score = np.clip(mouth_ratio / self.config.mouth_ar_thresh, 0, 1)\n",
        "            eye_score = np.clip(eye_ratio / self.config.eye_ar_thresh, 0, 1)\n",
        "            return float(0.7 * mouth_score + 0.3 * eye_score)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating happiness: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_neutral(self, eye_ratio: float, mouth_ratio: float, brow_ratio: float) -> float:\n",
        "        \"\"\"Calcula el score neutral basado en ratios faciales\"\"\"\n",
        "        try:\n",
        "            eye_diff = abs(eye_ratio - self.config.eye_ar_thresh)\n",
        "            mouth_diff = abs(mouth_ratio - self.config.mouth_ar_thresh)\n",
        "            brow_diff = abs(brow_ratio - 0.5)\n",
        "            return float(1 - np.clip((eye_diff + mouth_diff + brow_diff) / 3, 0, 1))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating neutral: {e}\")\n",
        "            return 1.0\n",
        "\n",
        "    def _calculate_sadness(self, mouth_ratio: float, brow_ratio: float) -> float:\n",
        "        \"\"\"Calcula el score de tristeza basado en ratios faciales\"\"\"\n",
        "        try:\n",
        "            mouth_score = 1 - np.clip(mouth_ratio / self.config.mouth_ar_thresh, 0, 1)\n",
        "            brow_score = np.clip(brow_ratio, 0, 1)\n",
        "            return float(0.6 * mouth_score + 0.4 * brow_score)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating sadness: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_surprise(self, eye_ratio: float, mouth_ratio: float) -> float:\n",
        "        \"\"\"Calcula el score de sorpresa basado en ratios faciales\"\"\"\n",
        "        try:\n",
        "            eye_score = np.clip(eye_ratio / self.config.eye_ar_thresh, 0, 1)\n",
        "            mouth_score = np.clip(mouth_ratio / self.config.mouth_ar_thresh, 0, 1)\n",
        "            return float(0.5 * (eye_score + mouth_score))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating surprise: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_anger(self, brow_ratio: float, eye_ratio: float) -> float:\n",
        "        \"\"\"Calcula el score de enojo basado en ratios faciales\"\"\"\n",
        "        try:\n",
        "            brow_score = 1 - np.clip(brow_ratio, 0, 1)\n",
        "            eye_score = 1 - np.clip(eye_ratio / self.config.eye_ar_thresh, 0, 1)\n",
        "            return float(0.7 * brow_score + 0.3 * eye_score)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating anger: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_attention(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el score de atención basado en ratios de ojos y pose de cabeza\"\"\"\n",
        "        try:\n",
        "            eye_attention = self._calculate_eye_ratio(landmarks)\n",
        "            head_pose = self._estimate_head_pose(landmarks)\n",
        "\n",
        "            attention_score = 0.7 * np.clip(eye_attention / self.config.eye_ar_thresh, 0, 1)\n",
        "            attention_score += 0.3 * (1 - abs(head_pose['pitch']) / math.pi)\n",
        "\n",
        "            return float(np.clip(attention_score, 0, 1))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating attention: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_engagement(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el engagement basado en el movimiento facial\"\"\"\n",
        "        try:\n",
        "            movement_score = self._calculate_face_movement(landmarks)\n",
        "            return float(np.clip(movement_score, 0, 1))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating engagement: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def _calculate_face_movement(self, landmarks: np.ndarray) -> float:\n",
        "        \"\"\"Calcula un score de movimiento facial basado en la variación de los landmarks\"\"\"\n",
        "        try:\n",
        "            center = np.mean(landmarks, axis=0)\n",
        "            distances = np.linalg.norm(landmarks - center, axis=1)\n",
        "            movement_score = np.std(distances)\n",
        "            normalized_score = np.clip(movement_score / 0.1, 0, 1)\n",
        "            return float(normalized_score)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating face movement: {e}\")\n",
        "            return 0.5\n",
        "\n",
        "    def _estimate_head_pose(self, landmarks: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Estima la pose de la cabeza en términos de pitch, yaw y roll\"\"\"\n",
        "        try:\n",
        "            if len(landmarks) < 468:\n",
        "                raise ValueError(f\"Insufficient landmarks for head pose estimation: {len(landmarks)}\")\n",
        "\n",
        "            nose = landmarks[1]\n",
        "            left_eye = landmarks[33]\n",
        "            right_eye = landmarks[263]\n",
        "\n",
        "            pitch = np.arctan2(nose[1] - (left_eye[1] + right_eye[1])/2,\n",
        "                               nose[2] - (left_eye[2] + right_eye[2])/2)\n",
        "\n",
        "            yaw = np.arctan2(nose[0] - (left_eye[0] + right_eye[0])/2,\n",
        "                             nose[2] - (left_eye[2] + right_eye[2])/2)\n",
        "\n",
        "            roll = np.arctan2(right_eye[1] - left_eye[1],\n",
        "                              right_eye[0] - left_eye[0])\n",
        "\n",
        "            return {\n",
        "                'pitch': float(np.clip(pitch, -np.pi/2, np.pi/2)),\n",
        "                'yaw': float(np.clip(yaw, -np.pi/2, np.pi/2)),\n",
        "                'roll': float(np.clip(roll, -np.pi/2, np.pi/2))\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error estimating head pose: {e}\")\n",
        "            return {'pitch': 0.0, 'yaw': 0.0, 'roll': 0.0}\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_segment_data(segment_result: Dict) -> bool:\n",
        "        \"\"\"Valida que el resultado del segmento tenga la estructura correcta\"\"\"\n",
        "        try:\n",
        "            required_keys = ['video_id', 'segment_info', 'facial_analysis', 'audio_analysis', 'combined_scores']\n",
        "            if not all(key in segment_result for key in required_keys):\n",
        "                return False\n",
        "\n",
        "            # Validar estructura interna\n",
        "            if not all(key in segment_result['segment_info'] for key in ['start_time', 'end_time', 'duration']):\n",
        "                return False\n",
        "\n",
        "            if not all(key in segment_result['facial_analysis'] for key in ['emotion', 'attention_score', 'engagement_score']):\n",
        "                return False\n",
        "\n",
        "            if not all(key in segment_result['audio_analysis'] for key in ['emotion', 'energy']):\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def extract_features(self, frame: np.ndarray) -> Optional[Dict]:\n",
        "        \"\"\"Extrae características faciales de un frame\"\"\"\n",
        "        if frame is None or frame.size == 0:\n",
        "            return self._get_empty_features()\n",
        "\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        try:\n",
        "            results = self.face_mesh.process(rgb_frame)\n",
        "            if not results.multi_face_landmarks:\n",
        "                return self._get_empty_features()\n",
        "\n",
        "            landmarks = self._extract_landmarks(results.multi_face_landmarks[0])\n",
        "            features = self._analyze_facial_features(landmarks, rgb_frame)\n",
        "\n",
        "            return features\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in feature extraction: {e}\")\n",
        "            return self._get_empty_features()\n",
        "\n",
        "    def _extract_landmarks(self, face_landmarks) -> np.ndarray:\n",
        "        \"\"\"Extrae los puntos de referencia faciales\"\"\"\n",
        "        landmarks = []\n",
        "        for lm in face_landmarks.landmark:\n",
        "            landmark = (lm.x, lm.y, lm.z)\n",
        "            landmarks.append(landmark)\n",
        "\n",
        "        # Verificar que haya suficientes landmarks\n",
        "        if len(landmarks) < 468:\n",
        "            # Rellenar con ceros si faltan landmarks\n",
        "            landmarks += [(0, 0, 0)] * (468 - len(landmarks))\n",
        "\n",
        "        return np.array(landmarks)\n",
        "\n",
        "    def _analyze_facial_features(self, landmarks: np.ndarray, frame: np.ndarray) -> Dict:\n",
        "        \"\"\"Analiza las características faciales\"\"\"\n",
        "        try:\n",
        "            eye_ratio = self._calculate_eye_ratio(landmarks)\n",
        "            mouth_ratio = self._calculate_mouth_ratio(landmarks)\n",
        "            brow_ratio = self._calculate_brow_ratio(landmarks)\n",
        "\n",
        "            emotions = self._analyze_emotions(eye_ratio, mouth_ratio, brow_ratio)\n",
        "            attention = self._calculate_attention(landmarks)\n",
        "            engagement = self._calculate_engagement(landmarks)\n",
        "\n",
        "            return {\n",
        "                'landmarks': landmarks.flatten(),\n",
        "                'emotions': emotions,\n",
        "                'attention_score': attention,\n",
        "                'engagement_score': engagement,\n",
        "                'eye_ratio': eye_ratio,\n",
        "                'mouth_ratio': mouth_ratio\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in facial feature analysis: {e}\")\n",
        "            return self._get_empty_features()\n",
        "\n",
        "    def extract_facial_landmarks(self, frame: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extrae los 468 landmarks faciales de un frame\"\"\"\n",
        "        results = self.face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if not results.multi_face_landmarks:\n",
        "            return np.zeros(468 * 3)\n",
        "\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            landmarks = np.array([(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark]).flatten()\n",
        "            return landmarks\n",
        "\n",
        "    def check_resources(self):\n",
        "        \"\"\"Verifica que todos los recursos necesarios estén disponibles\"\"\"\n",
        "        if not self.face_mesh:\n",
        "            raise RuntimeError(\"MediaPipe face mesh not initialized\")\n",
        "\n",
        "        if not os.path.exists(self.output_path):\n",
        "            raise FileNotFoundError(f\"Output directory not found: {self.output_path}\")\n",
        "\n",
        "    def _analyze_voice_characteristics(self, audio_segment: np.ndarray) -> Dict[str, float]:\n",
        "        try:\n",
        "            # Calcular claridad de voz\n",
        "            signal_power = np.mean(audio_segment ** 2)\n",
        "            noise_power = np.var(audio_segment)\n",
        "            clarity = 10 * np.log10(signal_power / (noise_power + 1e-10))\n",
        "            clarity_normalized = np.clip(clarity / 20, 0, 1)\n",
        "\n",
        "            # Calcular estabilidad vocal\n",
        "            correlation = np.correlate(audio_segment, audio_segment, mode='full')\n",
        "            middle = len(correlation) // 2\n",
        "            stability = np.sum(correlation[middle:middle+100]) / correlation[middle]\n",
        "            stability_normalized = np.clip(stability, 0, 1)\n",
        "\n",
        "            # Calcular variación de volumen\n",
        "            rms = librosa.feature.rms(y=audio_segment)[0]\n",
        "            volume_variation = np.std(rms) / (np.mean(rms) + 1e-10)\n",
        "            volume_variation_normalized = np.clip(volume_variation, 0, 1)\n",
        "\n",
        "            # Calcular tasa de habla\n",
        "            zcr = librosa.feature.zero_crossing_rate(y=audio_segment)[0]\n",
        "            speech_rate = np.mean(zcr)\n",
        "            speech_rate_normalized = np.clip(speech_rate * 2, 0, 1)\n",
        "\n",
        "            # Calcular rango de pitch\n",
        "            pitches, magnitudes = librosa.piptrack(y=audio_segment, sr=self.config.sample_rate)\n",
        "            pitch_range = np.ptp(pitches[magnitudes > 0])\n",
        "            pitch_range_normalized = np.clip(pitch_range / 100, 0, 1)\n",
        "\n",
        "            return {\n",
        "                'clarity': float(clarity_normalized),\n",
        "                'stability': float(stability_normalized),\n",
        "                'volume_variation': float(volume_variation_normalized),\n",
        "                'speech_rate': float(speech_rate_normalized),\n",
        "                'pitch_range': float(pitch_range_normalized)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing voice characteristics: {str(e)}\")\n",
        "            return {\n",
        "                'clarity': 0.0,\n",
        "                'stability': 0.0,\n",
        "                'volume_variation': 0.0,\n",
        "                'speech_rate': 0.0,\n",
        "                'pitch_range': 0.0\n",
        "            }\n",
        "\n",
        "    def _analyze_background_noise(self, audio_segment: np.ndarray) -> float:\n",
        "        \"\"\"Analiza el nivel de ruido de fondo\"\"\"\n",
        "        try:\n",
        "            # Calcular la energía promedio\n",
        "            rms = librosa.feature.rms(y=audio_segment)\n",
        "            # Usar el percentil 10 como estimación del ruido de fondo\n",
        "            noise_level = np.percentile(rms, 10)\n",
        "            return float(np.clip(noise_level, 0, 1))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error analyzing background noise: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _analyze_beat_alignment(self, audio_segment: np.ndarray) -> float:\n",
        "        \"\"\"Analiza el alineamiento con el beat musical\"\"\"\n",
        "        try:\n",
        "            tempo, beats = librosa.beat.beat_track(y=audio_segment, sr=self.config.sample_rate)\n",
        "            if len(beats) > 0:\n",
        "                # Calcular regularidad de los beats\n",
        "                beat_intervals = np.diff(beats)\n",
        "                regularity = 1.0 - np.std(beat_intervals) / np.mean(beat_intervals)\n",
        "                return float(np.clip(regularity, 0, 1))\n",
        "            return 0.0\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error analyzing beat alignment: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "class AudioAnalyzer:\n",
        "    \"\"\"Analiza los aspectos de audio del video\"\"\"\n",
        "    def __init__(self, config: AnalysisConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def _get_empty_features(self) -> Dict:\n",
        "        \"\"\"Retorna un diccionario vacío de características de audio\"\"\"\n",
        "        return {\n",
        "            'mfccs': [0.0] * self.config.mfcc_coeffs,\n",
        "            'mfcc_deltas': [0.0] * self.config.mfcc_coeffs,\n",
        "            'energy': 0.0,\n",
        "            'zcr': 0.0,\n",
        "            'spectral': {\n",
        "                'centroid': 0.0,\n",
        "                'rolloff': 0.0\n",
        "            },\n",
        "            'pitch': {\n",
        "                'mean': 0.0,\n",
        "                'std': 0.0,\n",
        "                'min': 0.0,\n",
        "                'max': 0.0,\n",
        "                'voiced_percent': 0.0\n",
        "            },\n",
        "            'emotions': {\n",
        "                'happy': 0.0,\n",
        "                'neutral': 1.0,\n",
        "                'sad': 0.0,\n",
        "                'angry': 0.0,\n",
        "                'excited': 0.0\n",
        "            },\n",
        "            'voice_characteristics': {\n",
        "                'clarity': 0.0,\n",
        "                'stability': 0.0,\n",
        "                'volume_variation': 0.0,\n",
        "                'speech_rate': 0.0,\n",
        "                'pitch_range': 0.0\n",
        "            },\n",
        "            'background_noise_level': 0.0,\n",
        "            'music_beat_alignment': 0.0\n",
        "        }\n",
        "\n",
        "    def extract_features(self, audio_segment: np.ndarray) -> Dict:\n",
        "        \"\"\"Extrae características del audio\"\"\"\n",
        "        if audio_segment is None or len(audio_segment) < self.config.hop_length:\n",
        "            return self._get_empty_features()\n",
        "\n",
        "        try:\n",
        "            # Pre-procesar el audio si es necesario\n",
        "            audio_segment = self._process_audio_segment(audio_segment)\n",
        "\n",
        "            # Extraer MFCCs\n",
        "            mfccs = librosa.feature.mfcc(\n",
        "                y=audio_segment,\n",
        "                sr=self.config.sample_rate,\n",
        "                n_mfcc=self.config.mfcc_coeffs,\n",
        "                n_fft=min(1024, len(audio_segment)),\n",
        "                hop_length=min(512, len(audio_segment) // 4),\n",
        "                window='hamming'\n",
        "            )\n",
        "\n",
        "            # Calcular características espectrales\n",
        "            spectral_features = self._extract_spectral_features(audio_segment)\n",
        "\n",
        "            # Análisis de pitch con ventana adaptativa\n",
        "            pitch_features = self._analyze_pitch(audio_segment)\n",
        "\n",
        "            # Análisis de emociones\n",
        "            emotion_features = self._analyze_audio_emotions(mfccs)\n",
        "\n",
        "            # Calcular características de voz\n",
        "            voice_features = self._analyze_voice_characteristics(audio_segment)\n",
        "\n",
        "            # Calcular nivel de ruido de fondo\n",
        "            background_noise = self._calculate_background_noise(audio_segment)\n",
        "\n",
        "            # Calcular alineamiento con el beat\n",
        "            beat_alignment = self._calculate_beat_alignment(audio_segment)\n",
        "\n",
        "            features = {\n",
        "                'mfccs': np.mean(mfccs.T, axis=0).tolist(),\n",
        "                'mfcc_deltas': np.mean(librosa.feature.delta(mfccs, width=3), axis=1).tolist(),\n",
        "                'energy': float(np.mean(librosa.feature.rms(y=audio_segment))),\n",
        "                'zcr': float(np.mean(librosa.feature.zero_crossing_rate(y=audio_segment))),\n",
        "                'spectral': spectral_features,\n",
        "                'pitch': pitch_features,\n",
        "                'emotions': emotion_features,\n",
        "                'voice_characteristics': voice_features,\n",
        "                'background_noise_level': float(background_noise),\n",
        "                'music_beat_alignment': float(beat_alignment)\n",
        "            }\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in audio feature extraction: {str(e)}\")\n",
        "            return self._get_empty_features()\n",
        "\n",
        "    def _extract_spectral_features(self, audio_segment: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Extrae características espectrales del audio\"\"\"\n",
        "        try:\n",
        "            # Configurar parámetros\n",
        "            n_fft = min(2048, len(audio_segment))\n",
        "            hop_length = min(512, len(audio_segment) // 4)\n",
        "\n",
        "            # Calcular centroides espectrales\n",
        "            centroids = librosa.feature.spectral_centroid(\n",
        "                y=audio_segment,\n",
        "                sr=self.config.sample_rate,\n",
        "                n_fft=n_fft,\n",
        "                hop_length=hop_length\n",
        "            )\n",
        "\n",
        "            # Calcular rolloff espectral\n",
        "            rolloff = librosa.feature.spectral_rolloff(\n",
        "                y=audio_segment,\n",
        "                sr=self.config.sample_rate,\n",
        "                n_fft=n_fft,\n",
        "                hop_length=hop_length,\n",
        "                roll_percent=0.85\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'centroid': float(np.mean(centroids)),\n",
        "                'rolloff': float(np.mean(rolloff))\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting spectral features: {str(e)}\")\n",
        "            return {'centroid': 0.0, 'rolloff': 0.0}\n",
        "\n",
        "    def _analyze_pitch(self, audio_segment: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Analiza características de pitch\"\"\"\n",
        "        try:\n",
        "            # Extraer pitch con librosa\n",
        "            pitches, magnitudes = librosa.piptrack(\n",
        "                y=audio_segment,\n",
        "                sr=self.config.sample_rate,\n",
        "                fmin=librosa.note_to_hz('C2'),\n",
        "                fmax=librosa.note_to_hz('C7')\n",
        "            )\n",
        "\n",
        "            # Filtrar pitches válidos\n",
        "            pitches_valid = pitches[magnitudes > np.max(magnitudes) * 0.1]\n",
        "\n",
        "            if len(pitches_valid) > 0:\n",
        "                return {\n",
        "                    'mean': float(np.mean(pitches_valid)),\n",
        "                    'std': float(np.std(pitches_valid)),\n",
        "                    'min': float(np.min(pitches_valid)),\n",
        "                    'max': float(np.max(pitches_valid)),\n",
        "                    'voiced_percent': float(len(pitches_valid) / pitches.size)\n",
        "                }\n",
        "            return {\n",
        "                'mean': 0.0,\n",
        "                'std': 0.0,\n",
        "                'min': 0.0,\n",
        "                'max': 0.0,\n",
        "                'voiced_percent': 0.0\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing pitch: {str(e)}\")\n",
        "            return {\n",
        "                'mean': 0.0,\n",
        "                'std': 0.0,\n",
        "                'min': 0.0,\n",
        "                'max': 0.0,\n",
        "                'voiced_percent': 0.0\n",
        "            }\n",
        "\n",
        "    def _analyze_audio_emotions(self, mfccs: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Analiza emociones en el audio usando MFCCs\"\"\"\n",
        "        try:\n",
        "            # Extraer estadísticas de los MFCCs\n",
        "            mfcc_mean = np.mean(mfccs, axis=1)\n",
        "            mfcc_std = np.std(mfccs, axis=1)\n",
        "\n",
        "            # Calcular características para cada emoción\n",
        "            energy = np.mean(mfcc_mean[:4])  # Primeros coeficientes\n",
        "            variability = np.mean(mfcc_std)   # Variabilidad general\n",
        "\n",
        "            # Mapear características a emociones\n",
        "            emotions = {\n",
        "                'happy': float(np.clip(0.5 + energy * 0.5, 0, 1)),\n",
        "                'neutral': float(np.clip(1 - variability, 0, 1)),\n",
        "                'sad': float(np.clip(0.5 - energy * 0.5, 0, 1)),\n",
        "                'angry': float(np.clip(variability, 0, 1)),\n",
        "                'excited': float(np.clip(energy + variability, 0, 1))\n",
        "            }\n",
        "\n",
        "            # Normalizar\n",
        "            total = sum(emotions.values())\n",
        "            if total > 0:\n",
        "                emotions = {k: v/total for k, v in emotions.items()}\n",
        "\n",
        "            return emotions\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing audio emotions: {str(e)}\")\n",
        "            return {\n",
        "                'happy': 0.0,\n",
        "                'neutral': 1.0,\n",
        "                'sad': 0.0,\n",
        "                'angry': 0.0,\n",
        "                'excited': 0.0\n",
        "            }\n",
        "\n",
        "    def _calculate_background_noise(self, audio_segment: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el nivel de ruido de fondo\"\"\"\n",
        "        try:\n",
        "            # Calcular RMS en ventanas pequeñas\n",
        "            frame_length = 2048\n",
        "            hop_length = 512\n",
        "            rms = librosa.feature.rms(y=audio_segment,\n",
        "                                    frame_length=frame_length,\n",
        "                                    hop_length=hop_length)[0]\n",
        "\n",
        "            # Usar el percentil 10 como estimación del ruido de fondo\n",
        "            noise_level = np.percentile(rms, 10)\n",
        "\n",
        "            # Normalizar a [0,1]\n",
        "            return float(np.clip(noise_level / np.max(rms), 0, 1))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating background noise: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_beat_alignment(self, audio_segment: np.ndarray) -> float:\n",
        "        \"\"\"Calcula el alineamiento con el beat musical\"\"\"\n",
        "        try:\n",
        "            # Detectar el tempo y los beats\n",
        "            tempo, beats = librosa.beat.beat_track(y=audio_segment,\n",
        "                                                 sr=self.config.sample_rate,\n",
        "                                                 start_bpm=120,\n",
        "                                                 units='frames')\n",
        "\n",
        "            if len(beats) > 1:\n",
        "                # Calcular la regularidad de los intervalos entre beats\n",
        "                beat_intervals = np.diff(beats)\n",
        "                regularity = 1.0 - (np.std(beat_intervals) / np.mean(beat_intervals))\n",
        "                return float(np.clip(regularity, 0, 1))\n",
        "            return 0.0\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating beat alignment: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _process_audio_segment(self, audio_segment: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Pre-procesa el segmento de audio\"\"\"\n",
        "        try:\n",
        "            # Asegurar longitud mínima\n",
        "            min_samples = self.config.n_fft + (self.config.mfcc_coeffs - 1) * self.config.hop_length\n",
        "            if len(audio_segment) < min_samples:\n",
        "                # Rellenar con reflexión para mantener las características\n",
        "                audio_segment = np.pad(audio_segment,\n",
        "                                     (0, min_samples - len(audio_segment)),\n",
        "                                     mode='reflect')\n",
        "\n",
        "            # Normalizar amplitud\n",
        "            audio_segment = librosa.util.normalize(audio_segment)\n",
        "\n",
        "            return audio_segment\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing audio segment: {str(e)}\")\n",
        "            return np.zeros(min_samples)\n",
        "\n",
        "    def _convert_to_serializable(self, obj):\n",
        "        \"\"\"Convierte objetos numpy a tipos Python nativos\"\"\"\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32,\n",
        "            np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return self._convert_to_serializable(obj.tolist())\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return [self._convert_to_serializable(x) for x in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: self._convert_to_serializable(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        return obj\n",
        "\n",
        "class TikTokAnalyzer:\n",
        "    \"\"\"Analizador completo de videos de TikTok que incluye análisis de video, audio, características faciales y generación de reportes.\"\"\"\n",
        "\n",
        "    def __init__(self, videos_path: str, output_path: str, config: AnalysisConfig):\n",
        "        \"\"\"\n",
        "        Inicializa el analizador de TikTok.\n",
        "\n",
        "        Args:\n",
        "            videos_path: Ruta al directorio con los videos\n",
        "            output_path: Ruta donde se guardarán los resultados\n",
        "            config: Configuración del análisis\n",
        "        \"\"\"\n",
        "        self.videos_path = Path(videos_path)\n",
        "        self.output_path = Path(output_path)\n",
        "        self.config = config\n",
        "\n",
        "        # Crear directorios necesarios\n",
        "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Inicializar analizadores\n",
        "        self.video_analyzer = VideoAnalyzer(\n",
        "            videos_path=str(self.videos_path),\n",
        "            output_path=str(self.output_path),\n",
        "            config=self.config\n",
        "        )\n",
        "        self.audio_analyzer = AudioAnalyzer(config=self.config)\n",
        "\n",
        "        # Inicializar modelos\n",
        "        self.initialize_models()\n",
        "\n",
        "        # Configurar logging\n",
        "        self._setup_logging()\n",
        "\n",
        "        # Inicializar archivos CSV\n",
        "        self._initialize_csv_files()\n",
        "\n",
        "        # Inicializar tiempo de inicio\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def _setup_logging(self) -> logging.Logger:\n",
        "        \"\"\"Configura el sistema de logging para el analizador de video\"\"\"\n",
        "        logger = logging.getLogger(f\"{__name__}.VideoAnalyzer\")\n",
        "\n",
        "        if not logger.handlers:\n",
        "            # Configurar formato\n",
        "            formatter = logging.Formatter(\n",
        "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "            )\n",
        "\n",
        "            # Crear archivo de log\n",
        "            log_file = self.output_path / 'video_analysis.log'\n",
        "            file_handler = logging.FileHandler(log_file)\n",
        "            file_handler.setFormatter(formatter)\n",
        "            file_handler.setLevel(logging.INFO)\n",
        "\n",
        "            # Handler de consola\n",
        "            console_handler = logging.StreamHandler()\n",
        "            console_handler.setFormatter(formatter)\n",
        "            console_handler.setLevel(logging.INFO)\n",
        "\n",
        "            # Agregar handlers al logger\n",
        "            logger.addHandler(file_handler)\n",
        "            logger.addHandler(console_handler)\n",
        "            logger.setLevel(logging.INFO)\n",
        "\n",
        "        return logger\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Inicializa los modelos necesarios para el análisis\"\"\"\n",
        "        try:\n",
        "            # Cargar modelos CNN y de texto\n",
        "            self.cnn_model = self._load_cnn_model()\n",
        "            self.text_model = self._load_text_model()\n",
        "            logger.info(\"Models initialized successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing models: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _initialize_csv_files(self):\n",
        "        \"\"\"Inicializa los archivos CSV para almacenar resultados\"\"\"\n",
        "        try:\n",
        "            # Headers para análisis de video\n",
        "            video_headers = [\n",
        "                'video_id', 'duration', 'avg_engagement', 'avg_attention',\n",
        "                'dominant_emotion', 'performance_score', 'visual_quality_score',\n",
        "                'scene_changes', 'motion_intensity', 'color_diversity',\n",
        "                'brightness_contrast', 'face_visibility_percent',\n",
        "                'audio_quality_score', 'music_presence', 'speech_clarity',\n",
        "                'audio_tempo', 'volume_variation', 'content_type',\n",
        "                'hashtag_count', 'text_overlay_count', 'effect_usage',\n",
        "                'transition_quality', 'estimated_virality_score',\n",
        "                'engagement_rate', 'retention_score', 'trend_alignment',\n",
        "                'upload_time', 'trending_category', 'similar_viral_videos'\n",
        "            ]\n",
        "            self._write_csv('video_analysis.csv', video_headers)\n",
        "\n",
        "            # Headers para análisis de segmentos\n",
        "            segment_headers = [\n",
        "                'video_id', 'segment_id', 'start_time', 'end_time', 'duration',\n",
        "                'facial_emotion', 'emotion_intensity', 'face_size_ratio',\n",
        "                'face_angle', 'gaze_direction', 'expression_change_rate',\n",
        "                'motion_vectors', 'camera_stability', 'scene_complexity',\n",
        "                'action_intensity', 'audio_emotion', 'voice_characteristics',\n",
        "                'background_noise_level', 'music_beat_alignment', 'audio_energy',\n",
        "                'text_overlay_content', 'effect_type', 'transition_type',\n",
        "                'object_detection', 'scene_classification',\n",
        "                'segment_engagement_score', 'attention_retention',\n",
        "                'viewer_response_prediction'\n",
        "            ]\n",
        "            self._write_csv('segment_analysis.csv', segment_headers)\n",
        "\n",
        "            logger.info(\"CSV files initialized successfully\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing CSV files: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _load_cnn_model(self):\n",
        "        \"\"\"Carga el modelo CNN para análisis de video\"\"\"\n",
        "        try:\n",
        "            # Verificar si el modelo existe\n",
        "            model_path = self.config.model_paths['cnn']\n",
        "\n",
        "            if not Path(model_path).exists():\n",
        "                logger.warning(f\"CNN model not found at {model_path}. Using fallback processing.\")\n",
        "                return None\n",
        "\n",
        "            model = load_model(model_path)\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading CNN model: {e}. Using fallback processing.\")\n",
        "            return None\n",
        "\n",
        "    def _load_text_model(self):\n",
        "        \"\"\"Carga el modelo de lenguaje para análisis de texto\"\"\"\n",
        "        try:\n",
        "            # Verificar si el modelo existe\n",
        "            model_path = self.config.model_paths['text']\n",
        "\n",
        "            if not Path(model_path).exists():\n",
        "                logger.warning(f\"Text model not found at {model_path}. Using fallback processing.\")\n",
        "                return None\n",
        "\n",
        "            with open(model_path, 'rb') as file:\n",
        "                model = pickle.load(file)\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error loading text model: {e}. Using fallback processing.\")\n",
        "            return None\n",
        "\n",
        "    def process_video(self, video_path: Path) -> Optional[Dict]:\n",
        "        \"\"\"Procesa un video individual\"\"\"\n",
        "        return self.video_analyzer.process_video(video_path)\n",
        "\n",
        "    def process_all_videos(self):\n",
        "        \"\"\"Procesa todos los videos en el directorio\"\"\"\n",
        "        video_files = list(self.videos_path.glob('*.mp4'))\n",
        "        results = []\n",
        "\n",
        "        for video_file in video_files:\n",
        "            result = self.process_video(video_file)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _write_segment_results(self, segment_result: Dict):\n",
        "        \"\"\"Escribe los resultados del segmento al CSV\"\"\"\n",
        "        self.video_analyzer._write_segment_results(segment_result)\n",
        "\n",
        "    def _write_video_analysis(self, analysis_data: Dict):\n",
        "        \"\"\"Escribe los resultados del análisis completo del video al CSV\"\"\"\n",
        "        self._write_csv('video_analysis.csv', list(analysis_data.keys()), analysis_data)\n",
        "\n",
        "    def _write_csv(self, filename: str, headers: List[str], data: Optional[Dict] = None):\n",
        "        \"\"\"Escribe o actualiza un archivo CSV\"\"\"\n",
        "        file_path = self.output_path / filename\n",
        "        mode = 'a' if file_path.exists() and data else 'w'\n",
        "\n",
        "        try:\n",
        "            with file_path.open(mode, newline='') as f:\n",
        "                writer = csv.DictWriter(f, fieldnames=headers)\n",
        "                if mode == 'w':\n",
        "                    writer.writeheader()\n",
        "                if data:\n",
        "                    writer.writerow(data)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error writing to CSV {filename}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_report(self) -> Dict:\n",
        "        \"\"\"Genera un reporte final del análisis\"\"\"\n",
        "        try:\n",
        "            report = {\n",
        "                'analysis_summary': self._generate_analysis_summary(),\n",
        "                'video_metrics': self._calculate_video_metrics(),\n",
        "                'segment_analysis': self._analyze_segments(),\n",
        "                'recommendations': self._generate_recommendations(),\n",
        "                'technical_details': {\n",
        "                    'processed_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'config_used': dataclasses.asdict(self.config),\n",
        "                    'version': '1.0.0'\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Guardar reporte como JSON\n",
        "            report_path = self.output_path / 'analysis_report.json'\n",
        "            with report_path.open('w') as f:\n",
        "                json.dump(self._convert_to_serializable(report), f, indent=4)\n",
        "\n",
        "            logger.info(f\"Report generated successfully: {report_path}\")\n",
        "            return report\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating report: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _generate_analysis_summary(self) -> Dict:\n",
        "        \"\"\"Genera un resumen del análisis realizado\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'total_videos_processed': len(list(self.videos_path.glob('*.mp4'))),\n",
        "                'average_duration': float(pd.read_csv(self.output_path / 'video_analysis.csv')['duration'].mean()),\n",
        "                'most_common_emotion': pd.read_csv(self.output_path / 'segment_analysis.csv')['facial_emotion'].mode()[0],\n",
        "                'average_engagement': float(pd.read_csv(self.output_path / 'video_analysis.csv')['avg_engagement'].mean()),\n",
        "                'analysis_duration': (time.time() - self.start_time) / 60  # En minutos\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating analysis summary: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_video_metrics(self) -> Dict:\n",
        "        \"\"\"Calcula métricas generales de los videos\"\"\"\n",
        "        try:\n",
        "            video_df = pd.read_csv(self.output_path / 'video_analysis.csv')\n",
        "            return {\n",
        "                'performance_metrics': {\n",
        "                    'avg_engagement': float(video_df['avg_engagement'].mean()),\n",
        "                    'avg_attention': float(video_df['avg_attention'].mean()),\n",
        "                    'emotional_consistency': float(video_df['emotional_consistency'].mean())\n",
        "                },\n",
        "                'technical_metrics': {\n",
        "                    'avg_video_quality': float(video_df['visual_quality_score'].mean()),\n",
        "                    'avg_audio_quality': float(video_df['audio_quality_score'].mean())\n",
        "                },\n",
        "                'content_metrics': {\n",
        "                    'trend_alignment': float(video_df['trend_alignment'].mean()),\n",
        "                    'virality_potential': float(video_df['estimated_virality_score'].mean())\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating video metrics: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _analyze_segments(self) -> Dict:\n",
        "        \"\"\"Analiza los segmentos procesados\"\"\"\n",
        "        try:\n",
        "            segment_df = pd.read_csv(self.output_path / 'segment_analysis.csv')\n",
        "            return {\n",
        "                'segment_distribution': segment_df['duration'].value_counts(bins=5).to_dict(),\n",
        "                'peak_engagement_moments': segment_df.nlargest(5, 'segment_engagement_score')['segment_id'].tolist(),\n",
        "                'attention_patterns': segment_df.groupby('scene_classification')['attention_retention'].mean().to_dict(),\n",
        "                'emotional_progression': segment_df.groupby('segment_id')['facial_emotion'].agg(lambda x: x.value_counts().index[0]).to_dict()\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing segments: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _generate_recommendations(self) -> List[Dict]:\n",
        "        \"\"\"Genera recomendaciones basadas en el análisis\"\"\"\n",
        "        try:\n",
        "            video_df = pd.read_csv(self.output_path / 'video_analysis.csv')\n",
        "            segment_df = pd.read_csv(self.output_path / 'segment_analysis.csv')\n",
        "\n",
        "            recommendations = []\n",
        "\n",
        "            # Recomendaciones de engagement\n",
        "            if video_df['avg_engagement'].mean() < RECOMMENDATION_THRESHOLDS['ENGAGEMENT']:\n",
        "                recommendations.append({\n",
        "                    'category': 'engagement',\n",
        "                    'suggestion': 'Increase viewer engagement by adding more interactive elements or calls to action'\n",
        "                })\n",
        "\n",
        "            # Recomendaciones de consistencia emocional\n",
        "            if video_df['emotional_consistency'].mean() < RECOMMENDATION_THRESHOLDS['EMOTIONAL_CONSISTENCY']:\n",
        "                recommendations.append({\n",
        "                    'category': 'emotional_consistency',\n",
        "                    'suggestion': 'Improve emotional consistency by maintaining a coherent tone and narrative throughout the video'\n",
        "                })\n",
        "\n",
        "            # Recomendaciones de atención\n",
        "            if video_df['avg_attention'].mean() < RECOMMENDATION_THRESHOLDS['ATTENTION']:\n",
        "                recommendations.append({\n",
        "                    'category': 'attention',\n",
        "                    'suggestion': 'Retain viewer attention by using engaging visuals, dynamic editing, and compelling storytelling'\n",
        "                })\n",
        "\n",
        "            # Recomendaciones basadas en análisis de segmentos\n",
        "            low_engagement_segments = segment_df[segment_df['segment_engagement_score'] < 0.3]\n",
        "            if len(low_engagement_segments) > 0:\n",
        "                recommendations.append({\n",
        "                    'category': 'segment_optimization',\n",
        "                    'suggestion': f\"Optimize low engagement segments: {', '.join(low_engagement_segments['segment_id'].tolist())}\"\n",
        "                })\n",
        "\n",
        "            return recommendations\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating recommendations: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _convert_to_serializable(self, obj):\n",
        "        \"\"\"Convierte objetos a formato serializable para JSON\"\"\"\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32,\n",
        "            np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return self._convert_to_serializable(obj.tolist())\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return [self._convert_to_serializable(x) for x in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: self._convert_to_serializable(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, np.bool_):\n",
        "            return bool(obj)\n",
        "        elif isinstance(obj, (str, int, float, bool, type(None))):\n",
        "            return obj\n",
        "        return str(obj)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Limpia recursos y archivos temporales\"\"\"\n",
        "        try:\n",
        "            # Cerrar manejadores de archivos\n",
        "            handlers = logger.handlers[:]\n",
        "            for handler in handlers:\n",
        "                handler.close()\n",
        "                logger.removeHandler(handler)\n",
        "\n",
        "            # Limpiar caché si existe\n",
        "            if hasattr(self, 'mp_face_mesh'):\n",
        "                self.mp_face_mesh.close()\n",
        "\n",
        "            # Limpiar otros recursos si es necesario\n",
        "\n",
        "            self.logger.info(\"Cleanup completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during cleanup: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Soporte para context manager\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Limpieza al salir del context manager\"\"\"\n",
        "        self.cleanup()\n",
        "        if exc_type is not None:\n",
        "            logger.error(f\"Error during execution: {exc_type.__name__}: {exc_val}\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _prepare_features(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Prepara características para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'visual_features': self._extract_visual_features(analysis_results),\n",
        "                'audio_features': self._extract_audio_features(analysis_results),\n",
        "                'content_features': self._extract_content_features(analysis_results)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing features: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _prepare_labels(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Prepara etiquetas para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'engagement_score': analysis_results.get('engagement_score', 0.0),\n",
        "                'performance_score': analysis_results.get('performance_score', 0.0),\n",
        "                'virality_score': analysis_results.get('virality_score', 0.0)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing labels: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _prepare_metadata(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Prepara metadata para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'video_id': analysis_results.get('video_id', ''),\n",
        "                'duration': analysis_results.get('duration', 0.0),\n",
        "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'config_version': '1.0.0'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing metadata: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _prepare_comments(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Prepara comentarios para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'comment_count': 0,\n",
        "                'sentiment_distribution': {},\n",
        "                'key_topics': []\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing comments: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _prepare_metrics(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Prepara métricas para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'technical_metrics': {\n",
        "                    'video_quality': analysis_results.get('video_quality', 0.0),\n",
        "                    'audio_quality': analysis_results.get('audio_quality', 0.0)\n",
        "                },\n",
        "                'engagement_metrics': {\n",
        "                    'view_retention': analysis_results.get('view_retention', 0.0),\n",
        "                    'interaction_rate': analysis_results.get('interaction_rate', 0.0)\n",
        "                },\n",
        "                'performance_metrics': {\n",
        "                    'content_score': analysis_results.get('content_score', 0.0),\n",
        "                    'technical_score': analysis_results.get('technical_score', 0.0)\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing metrics: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_visual_features(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Extrae características visuales para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'facial_features': analysis_results.get('facial_features', {}),\n",
        "                'motion_features': analysis_results.get('motion_features', {}),\n",
        "                'scene_features': analysis_results.get('scene_features', {})\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting visual features: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_audio_features(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Extrae características de audio para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'spectral_features': analysis_results.get('spectral_features', {}),\n",
        "                'temporal_features': analysis_results.get('temporal_features', {}),\n",
        "                'rhythm_features': analysis_results.get('rhythm_features', {})\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting audio features: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _extract_content_features(self, analysis_results: Dict) -> Dict:\n",
        "        \"\"\"Extrae características de contenido para entrenamiento\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'hashtags': analysis_results.get('hashtags', []),\n",
        "                'text_overlay': analysis_results.get('text_overlay', []),\n",
        "                'effects': analysis_results.get('effects', [])\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting content features: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def get_status(self) -> Dict:\n",
        "        \"\"\"Retorna el estado actual del analizador\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'initialized': True,\n",
        "                'models_loaded': {\n",
        "                    'cnn_model': self.cnn_model is not None,\n",
        "                    'text_model': self.text_model is not None\n",
        "                },\n",
        "                'gpu_available': self.config.use_gpu,\n",
        "                'output_path_ready': self.output_path.exists(),\n",
        "                'last_analysis_time': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'processed_videos_count': len(list(self.output_path.glob('*.csv'))),\n",
        "                'config': dataclasses.asdict(self.config)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting status: {str(e)}\")\n",
        "            return {'initialized': False, 'error': str(e)}\n",
        "\n",
        "    def update_config(self, **kwargs) -> bool:\n",
        "        \"\"\"Actualiza la configuración del analizador\"\"\"\n",
        "        try:\n",
        "            for key, value in kwargs.items():\n",
        "                if hasattr(self.config, key):\n",
        "                    setattr(self.config, key, value)\n",
        "                else:\n",
        "                    logger.warning(f\"Unknown configuration parameter: {key}\")\n",
        "\n",
        "            # Validar nueva configuración\n",
        "            self.config.__post_init__()\n",
        "            logger.info(\"Configuration updated successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error updating configuration: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def reset(self) -> bool:\n",
        "        \"\"\"Resetea el analizador a su estado inicial\"\"\"\n",
        "        try:\n",
        "            # Cerrar recursos actuales\n",
        "            self.cleanup()\n",
        "\n",
        "            # Reinicializar componentes\n",
        "            self.initialize_models()\n",
        "            self._setup_logging()\n",
        "            self._initialize_csv_files()\n",
        "\n",
        "            logger.info(\"Analyzer reset successfully\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error resetting analyzer: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def get_performance_metrics(self) -> Dict:\n",
        "        \"\"\"Obtiene métricas de rendimiento del analizador\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                'processing_times': {\n",
        "                    'video_analysis': self._calculate_average_processing_time(),\n",
        "                    'facial_analysis': self._calculate_facial_analysis_time(),\n",
        "                    'audio_analysis': self._calculate_audio_analysis_time()\n",
        "                },\n",
        "                'memory_usage': {\n",
        "                    'current': psutil.Process().memory_info().rss / 1024 / 1024,  # MB\n",
        "                    'peak': self._get_peak_memory_usage()\n",
        "                },\n",
        "                'gpu_usage': self._get_gpu_usage() if self.config.use_gpu else None,\n",
        "                'processed_data': {\n",
        "                    'total_videos': self._count_processed_videos(),\n",
        "                    'total_segments': self._count_processed_segments(),\n",
        "                    'total_duration': self._calculate_total_duration()\n",
        "                }\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting performance metrics: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_average_processing_time(self) -> float:\n",
        "        \"\"\"Calcula el tiempo promedio de procesamiento\"\"\"\n",
        "        try:\n",
        "            processing_times = []\n",
        "            for video_file in self.videos_path.glob('*.mp4'):\n",
        "                start_time = time.time()\n",
        "                self._process_single_video(video_file)\n",
        "                processing_time = time.time() - start_time\n",
        "                processing_times.append(processing_time)\n",
        "\n",
        "            if processing_times:\n",
        "                avg_processing_time = np.mean(processing_times)\n",
        "            else:\n",
        "                avg_processing_time = 0.0\n",
        "\n",
        "            return float(avg_processing_time)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating processing time: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_facial_analysis_time(self) -> float:\n",
        "        \"\"\"Calcula el tiempo promedio de análisis facial\"\"\"\n",
        "        try:\n",
        "            facial_analysis_times = []\n",
        "            for video_file in self.videos_path.glob('*.mp4'):\n",
        "                cap = cv2.VideoCapture(str(video_file))\n",
        "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                frames_per_segment = int(cap.get(cv2.CAP_PROP_FPS) * self.config.segment_duration)\n",
        "\n",
        "                for frame_start in range(0, total_frames, frames_per_segment):\n",
        "                    start_time = time.time()\n",
        "                    frames = self._extract_frames(cap, frame_start, frames_per_segment)\n",
        "                    facial_features = self.video_analyzer.extract_facial_features(frames)\n",
        "                    facial_analysis_time = time.time() - start_time\n",
        "                    facial_analysis_times.append(facial_analysis_time)\n",
        "\n",
        "                cap.release()\n",
        "\n",
        "            if facial_analysis_times:\n",
        "                avg_facial_analysis_time = np.mean(facial_analysis_times)\n",
        "            else:\n",
        "                avg_facial_analysis_time = 0.0\n",
        "\n",
        "            return float(avg_facial_analysis_time)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating facial analysis time: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _calculate_audio_analysis_time(self) -> float:\n",
        "        \"\"\"Calcula el tiempo promedio de análisis de audio\"\"\"\n",
        "        try:\n",
        "            audio_analysis_times = []\n",
        "            for video_file in self.videos_path.glob('*.mp4'):\n",
        "                start_time = time.time()\n",
        "                audio, _ = librosa.load(str(video_file), sr=self.config.sample_rate)\n",
        "                audio_features = self.audio_analyzer.extract_features(audio)\n",
        "                audio_analysis_time = time.time() - start_time\n",
        "                audio_analysis_times.append(audio_analysis_time)\n",
        "\n",
        "            if audio_analysis_times:\n",
        "                avg_audio_analysis_time = np.mean(audio_analysis_times)\n",
        "            else:\n",
        "                avg_audio_analysis_time = 0.0\n",
        "\n",
        "            return float(avg_audio_analysis_time)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating audio analysis time: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _get_peak_memory_usage(self) -> float:\n",
        "        \"\"\"Obtiene el uso máximo de memoria\"\"\"\n",
        "        try:\n",
        "            peak_memory_usage = 0.0\n",
        "            for video_file in self.videos_path.glob('*.mp4'):\n",
        "                tracemalloc.start()\n",
        "                self._process_single_video(video_file)\n",
        "                current, peak = tracemalloc.get_traced_memory()\n",
        "                peak_memory_usage = max(peak_memory_usage, peak / 1024 / 1024)  # Convert to MB\n",
        "                tracemalloc.stop()\n",
        "\n",
        "            return float(peak_memory_usage)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting peak memory usage: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _get_gpu_usage(self) -> Dict:\n",
        "        \"\"\"Obtiene el uso de GPU si está disponible\"\"\"\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                return {\n",
        "                    'device_name': torch.cuda.get_device_name(0),\n",
        "                    'memory_allocated': torch.cuda.memory_allocated(0) / 1024 / 1024,  # MB\n",
        "                    'memory_cached': torch.cuda.memory_reserved(0) / 1024 / 1024  # MB\n",
        "                }\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting GPU usage: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _count_processed_videos(self) -> int:\n",
        "        \"\"\"Cuenta el número de videos procesados\"\"\"\n",
        "        try:\n",
        "            return len(list(self.output_path.glob('*.csv')))\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error counting processed videos: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def _count_processed_segments(self) -> int:\n",
        "        \"\"\"Cuenta el número total de segmentos procesados\"\"\"\n",
        "        try:\n",
        "            return int(pd.read_csv(self.output_path / 'segment_analysis.csv').shape[0])\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error counting processed segments: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def _calculate_total_duration(self) -> float:\n",
        "        \"\"\"Calcula la duración total de videos procesados\"\"\"\n",
        "        try:\n",
        "            return float(pd.read_csv(self.output_path / 'video_analysis.csv')['duration'].sum())\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating total duration: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "    def __init__(self, videos_path: str, output_path: str, config: AnalysisConfig):\n",
        "            \"\"\"\n",
        "            Inicializa el analizador de TikTok.\n",
        "\n",
        "            Args:\n",
        "                videos_path: Ruta al directorio con los videos\n",
        "                output_path: Ruta donde se guardarán los resultados\n",
        "                config: Configuración del análisis\n",
        "            \"\"\"\n",
        "            self.videos_path = Path(videos_path)\n",
        "            self.output_path = Path(output_path)\n",
        "            self.config = config\n",
        "\n",
        "            # Crear directorios necesarios\n",
        "            self.output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Inicializar analizadores - CORREGIDO\n",
        "            self.video_analyzer = VideoAnalyzer(\n",
        "                videos_path=str(self.videos_path),\n",
        "                output_path=str(self.output_path),\n",
        "                config=self.config\n",
        "            )\n",
        "            self.audio_analyzer = AudioAnalyzer(config=self.config)\n",
        "\n",
        "            # Inicializar modelos\n",
        "            self.initialize_models()\n",
        "\n",
        "            # Configurar logging\n",
        "            self._setup_logging()\n",
        "\n",
        "            # Inicializar archivos CSV\n",
        "            self._initialize_csv_files()\n",
        "\n",
        "            # Inicializar tiempo de inicio\n",
        "            self.start_time = time.time()\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Configurar rutas de manera más robusta\n",
        "        current_dir = Path.cwd()\n",
        "        videos_path = Path(r\"C:\\Users\\KR\\Documents\\muestra\")\n",
        "        output_path = Path(r\"C:\\Users\\KR\\Documents\\muestras\\Analysis_Result\")\n",
        "        models_path = Path(r\"C:\\Users\\KR\\Documents\\muestra\\models\")\n",
        "\n",
        "        # Crear directorios necesarios\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "        models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Configurar logging básico\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Crear configuración\n",
        "        config = AnalysisConfig(\n",
        "            sample_rate=16000,\n",
        "            segment_duration=2.5,\n",
        "            min_detection_confidence=0.5,\n",
        "            use_gpu=torch.cuda.is_available(),\n",
        "            batch_size=32,\n",
        "            cache_dir=str(output_path / \".cache\"),\n",
        "            model_paths={\n",
        "                'cnn': str(models_path / \"cnn_model.h5\"),\n",
        "                'text': str(models_path / \"text_model.pkl\")\n",
        "            }\n",
        "        )\n",
        "\n",
        "        logger.info(\"Initializing TikTok Analyzer...\")\n",
        "\n",
        "        analyzer = TikTokAnalyzer(\n",
        "            videos_path=str(videos_path),\n",
        "            output_path=str(output_path),\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        if analyzer.verify_setup():\n",
        "            # Procesar videos\n",
        "            results = analyzer.process_all_videos()\n",
        "            # Generar reporte\n",
        "            report = analyzer.generate_report()\n",
        "            logger.info(\"Analysis completed successfully\")\n",
        "            return results, report\n",
        "\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        logger.error(f\"Error in main execution: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        results, report = main()\n",
        "        print(\"Análisis completado con éxito\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante el análisis: {str(e)}\")\n",
        "        traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}